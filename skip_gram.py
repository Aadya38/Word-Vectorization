# -*- coding: utf-8 -*-
"""skip-gram.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EiInbFcPntI0FqcUGGejgOll6t3KbzXL
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from collections import Counter
import string
import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# Load data
df = pd.read_csv('train.csv').head(20000)


# Preprocess text
def preprocess_text(text):
    tokens = word_tokenize(text.lower())
    cleaned_tokens = []
    for word in tokens:
        cleaned_tokens.append(word.translate(str.maketrans('', '', string.punctuation)))
    return cleaned_tokens


df['Description'] = df['Description'].apply(preprocess_text)


# Build vocabulary
word_counts = Counter()
for description in df['Description']:
    for word in description:
        word_counts[word] += 1

vocab = {word: index for index, (word, _) in enumerate(word_counts.most_common())}
idx_to_word = {index: word for word, index in vocab.items()}


# Generate samples
def generate_samples(description, window_size, num_negative_samples, vocab):
    samples = []
    for i in range(len(description)):
        target_word = description[i]
        context_words = [description[j] for j in range(max(0, i - window_size), min(len(description), i + window_size + 1)) if j != i]
        for context_word in context_words:
            samples.append((vocab[target_word], vocab[context_word], 1))
            for _ in range(num_negative_samples):
                negative_word = np.random.choice(len(vocab))
                while negative_word == vocab[context_word]:
                    negative_word = np.random.choice(len(vocab))
                samples.append((vocab[target_word], negative_word, 0))
    return samples


# Define dataset
class WordEmbeddingDataset(Dataset):
    def __init__(self, descriptions, window_size, num_negative_samples, vocab):
        self.samples = []
        for description in descriptions:
            self.samples.extend(generate_samples(description, window_size, num_negative_samples, vocab))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        return self.samples[idx]


# Define model
class SkipGramNegSampling(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(SkipGramNegSampling, self).__init__()
        self.target_embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.init_emb()

    def init_emb(self):
        init_range = 0.5 / self.target_embeddings.embedding_dim
        self.target_embeddings.weight.data.uniform_(-init_range, init_range)
        self.context_embeddings.weight.data.uniform_(-0, 0)

    def forward(self, target_words, context_words):
        target_emb = self.target_embeddings(target_words)
        context_emb = self.context_embeddings(context_words)
        scores = torch.matmul(target_emb, context_emb.transpose(0, 1))
        return scores


# Hyperparameters
window_size = 5
num_negative_samples = 5
embedding_dim = 300
learning_rate = 0.001
batch_size = 1024
num_epochs = 5


# Create dataset and dataloader
dataset = WordEmbeddingDataset(df['Description'], window_size, num_negative_samples, vocab)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)


# Initialize model, loss function, and optimizer
model = SkipGramNegSampling(len(vocab), embedding_dim).to(device)
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)


# Training loop
for epoch in range(num_epochs):
    total_loss = 0
    for batch_idx, batch in enumerate(dataloader):
        target_words, context_words, labels = batch
        target_words, context_words, labels = target_words.to(device), context_words.to(device), labels.to(device)

        optimizer.zero_grad()
        scores = model(target_words, context_words)
        loss = criterion(scores.view(-1), labels.float())
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        if (batch_idx + 1) % 100 == 0:
            avg_loss = total_loss / (batch_idx + 1)
            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\n')


# Save word embeddings
word_embeddings = model.target_embeddings.weight.detach().cpu().numpy()
torch.save({'word_vectors': torch.tensor(word_embeddings), 'vocab': vocab}, 'skip-gram-word-vectors.pt')
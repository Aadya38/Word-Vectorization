# -*- coding: utf-8 -*-
"""svd.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13I9DzXZOIiII2nGYo8la1q41jnfH50yo
"""

import pandas as pd
import numpy as np
import nltk
import string
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import os
from nltk.tokenize import sent_tokenize, word_tokenize
from sklearn.preprocessing import normalize
from sklearn.model_selection import train_test_split
from tqdm import tqdm
from scipy.sparse.linalg import svds as sparse_svds
nltk.download('punkt')

def preprocess_text(text):
    text = " ".join(text)
    text = text.lower()

    # Convert the text into lowercase and replace punctuation marks with spaces
    text_chars = []
    for char in text:
        if char not in string.punctuation:
            text_chars.append(char)
        else:
            text_chars.append(' ')

    preprocessed_text = "".join(text_chars)
    words = preprocessed_text.split()
    cleaned_text = ' '.join(words)

    return cleaned_text

# Reading data
train_csv_path = "train.csv"
train_df = pd.read_csv(train_csv_path)
train_descriptions = train_df['Description'].tolist()
train_labels = train_df['Class Index'].tolist()

test_csv_path = "test.csv"
test_df = pd.read_csv(test_csv_path)
test_descriptions = test_df['Description'].tolist()
test_labels = test_df['Class Index'].tolist()

# Preprocessing data
preprocessed_train_sentences = []
encoded_train_labels = []
for train_sentence, train_label in tqdm(zip(train_descriptions, train_labels)):
    train_sentence_tokenized = sent_tokenize(train_sentence)
    preprocessed_train_sentence = preprocess_text(train_sentence_tokenized)
    preprocessed_train_sentences.append(preprocessed_train_sentence)
    encoded_train_labels.append(train_label)

preprocessed_test_sentences = []
encoded_test_labels = []
for test_sentence, test_label in tqdm(zip(test_descriptions, test_labels)):
    test_sentence_tokenized = sent_tokenize(test_sentence)
    preprocessed_test_sentence = preprocess_text(test_sentence_tokenized)
    preprocessed_test_sentences.append(preprocessed_test_sentence)
    encoded_test_labels.append(test_label)

def build_vocab(data):
    all_words = []
    temp_word_count = {}

    for sentence in tqdm(data):
        sentence_tokens = word_tokenize(sentence)
        sentence_tokens = ['<START>'] + sentence_tokens + ['<END>']

        for word in sentence_tokens:
            all_words.append(word)
            temp_word_count[word] = temp_word_count.get(word,0)+1

    vocab_dict = {'UNK': 0}
    for word, count in temp_word_count.items():
        vocab_dict[word] = count

    vocab = list(vocab_dict.keys())

    word_idx = {word: idx for idx, word in enumerate(vocab)}
    idx_word = {idx: word for idx, word in enumerate(vocab)}

    for i in tqdm(range(len(data))):
        sentence = data[i]
        sentence_tokens = word_tokenize(sentence)
        sentence_tokens = ['<START>'] + sentence_tokens + ['<END>']
        data[i] = ' '.join(sentence_tokens)

    return data, vocab_dict, vocab, word_idx, idx_word

# Limit the number of preprocessed sentences to 20,000
selected_preprocessed_sentences = preprocessed_train_sentences[:20000]

# Build vocabulary based on the selected preprocessed sentences
data_subset, vocab_dict, vocab, word_idx, idx_word = build_vocab(selected_preprocessed_sentences)

# Limit the number of labels to 20,000
selected_labels = train_labels[:20000]

# Creating cooccurrence matrix
def build_co_occurrence_matrix(data, vocab_dict, vocab, word_idx, idx_word, threshold=3, context_window=5):
    co_occurrence_matrix = np.zeros((len(vocab), len(vocab)))

    for i in tqdm(range(len(data))):
        sentence = data[i].split()
        for idx, word in enumerate(sentence):
            c_start = max(0, idx - context_window)
            c_end = min(len(sentence) - 1, idx + context_window)

            for j in range(c_start, c_end + 1):
                if idx == j:
                    continue
                co_occurrence_matrix[word_idx[word]][word_idx[sentence[j]]] += 1

    return co_occurrence_matrix



co_occurance_matrix = build_co_occurrence_matrix(data_subset , vocab_dict , vocab , word_idx , idx_word)

# Building SVD
def build_svd(co_occurrence_matrix, embedding_dim=200):
    U, S, VT = sparse_svds(co_occurrence_matrix, k=embedding_dim)
    return U, S, VT



embeddings_file_path = 'svd-word-vectors.pt'
U, S, VT = build_svd(co_occurance_matrix)
embedding = U
torch.save(U, embeddings_file_path)
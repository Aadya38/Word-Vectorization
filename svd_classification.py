# -*- coding: utf-8 -*-
"""svd-classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/123dpvh56pnaPVo9H0JhI795sl2EFwZOk
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import pandas as pd
import numpy as np
import string
from collections import Counter
from tqdm import tqdm
import torch.nn.utils.rnn as rnn_utils

class LSTMClassifier(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(LSTMClassifier, self).__init__()
        self.hidden_size = hidden_size
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        _, (h_n, _) = self.lstm(x)
        out = self.fc(h_n[-1])
        return out

svd_data = torch.load('svd-word-vectors.pt', map_location=device)

train_sentences = selected_preprocessed_sentences
test_sentences = preprocessed_test_sentences
train_labels = selected_labels
test_labels = encoded_test_labels

# Determine the maximum length of sentences
max_length = max(len(sentence.split()) for sentence in selected_preprocessed_sentences)

# Create custom datasets
train_dataset = SVD_Dataset(train_sentences, train_labels, embedding, word_idx, max_length)
test_dataset = SVD_Dataset(test_sentences, test_labels, embedding, word_idx, max_length)

# Create data loaders
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

class CustomDataset(Dataset):
    def __init__(self, data, word_vectors, vocab, max_len):
        self.data = data
        self.word_vectors = word_vectors
        self.vocab = vocab
        self.max_len = max_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        description = self.data.iloc[idx]['Description']
        label = self.data.iloc[idx]['Class Index'] - 1
        tokenized_desc = self.preprocess_text(description)
        desc_vector = self.get_sentence_vector(tokenized_desc)
        return desc_vector, label

    def preprocess_text(self, text):
        text = text.lower()
        text = text.translate(str.maketrans('', '', string.punctuation))
        return text.split()

    def get_sentence_vector(self, tokens):
        vector = torch.zeros(self.max_len, self.word_vectors.shape[1], device=device)
        for i, token in enumerate(tokens):
            if i >= self.max_len:
                break
            if token in self.vocab:
                vector[i] = torch.tensor(self.word_vectors[self.vocab[token]], device=device)
        return rnn_utils.pad_sequence([vector], batch_first=True).squeeze(0)

# Determine the embedding dimension
embedding_dim = len(embedding[0])
# Define the input size as the embedding dimension
input_size = embedding_dim
# Define the hidden size
hidden_size = 128
# Determine the number of classes (unique labels in the training set)
num_classes = len(set(train_labels)) + 1

# Instantiate the custom RNN classifier
model = CustomRNNClassifier(input_size, hidden_size, num_classes)
# Define the loss function
criterion = nn.CrossEntropyLoss()
# Define the optimizer
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Check if CUDA is available and move the model to the appropriate device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Define the number of epochs
num_epochs = 10

# Training loop
for epoch in range(num_epochs):
    # Set the model to training mode
    model.train()
    running_loss = 0.0
    correct_predictions = 0
    total_samples = 0
    for inputs, labels in tqdm(train_loader):
        inputs, labels = inputs.to(device), labels.to(device)

        # Clear gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(inputs)

        # Calculate the loss
        loss = criterion(outputs, labels)

        # Backward pass
        loss.backward()

        # Update weights
        optimizer.step()

        # Update running loss
        running_loss += loss.item() * inputs.size(0)
        _, predicted = torch.max(outputs, 1)

        # Update correct predictions
        correct_predictions += (predicted == labels).sum().item()

        # Update total samples
        total_samples += labels.size(0)

    # Calculate epoch statistics
    epoch_loss = running_loss / total_samples
    epoch_accuracy = (correct_predictions / total_samples) * 100.0
    if epoch+1==num_epochs:
      print()
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\n")

# Set the model to evaluation mode
model.eval()

# Initialize variables for calculating accuracy
correct_predictions = 0
total_samples = 0

# No need for torch.no_grad() context manager for evaluation
with torch.no_grad():
    for inputs, labels in test_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        _, predicted = torch.max(outputs, 1)
        correct_predictions += (predicted == labels).sum().item()
        total_samples += labels.size(0)

# Calculate accuracy
accuracy = correct_predictions / total_samples
print(f"Accuracy on the test set: {accuracy*100}")



torch.save(model.state_dict(), 'svd-classification-model.pt')

from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix

# Set model to evaluation mode
model.eval()

# Initialize variables
train_predicted_labels = []
train_true_labels = []
test_predicted_labels = []
test_true_labels = []

# No need for torch.no_grad() context manager for evaluation
for loader, predicted_labels, true_labels in [(train_loader, train_predicted_labels, train_true_labels), (test_loader, test_predicted_labels, test_true_labels)]:
    for inputs, labels in loader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        _, predicted = torch.max(outputs, 1)
        predicted_labels.extend(predicted.cpu().numpy())
        true_labels.extend(labels.cpu().numpy())

# Convert lists to numpy arrays
train_predicted_labels = np.array(train_predicted_labels)
train_true_labels = np.array(train_true_labels)
test_predicted_labels = np.array(test_predicted_labels)
test_true_labels = np.array(test_true_labels)

# Compute accuracy, precision, recall, F1 score, and confusion matrix
train_accuracy = accuracy_score(train_true_labels, train_predicted_labels)
train_precision, train_recall, train_f1, _ = precision_recall_fscore_support(train_true_labels, train_predicted_labels, average='weighted')
train_confusion_matrix = confusion_matrix(train_true_labels, train_predicted_labels)

test_accuracy = accuracy_score(test_true_labels, test_predicted_labels)
test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(test_true_labels, test_predicted_labels, average='weighted')
test_confusion_matrix = confusion_matrix(test_true_labels, test_predicted_labels)

# Print results for train set
print("Train Set:")
print(f"Accuracy: {train_accuracy}")
print(f"Precision: {train_precision}")
print(f"Recall: {train_recall}")
print(f"F1 Score: {train_f1}")
print("Confusion Matrix:")
print(train_confusion_matrix)

# Print results for test set
print("\nTest Set:")
print(f"Accuracy: {test_accuracy}")
print(f"Precision: {test_precision}")
print(f"Recall: {test_recall}")
print(f"F1 Score: {test_f1}")
print("Confusion Matrix:")
print(test_confusion_matrix)